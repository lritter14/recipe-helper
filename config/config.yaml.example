# Recipe Ingestion Pipeline Configuration
# Copy this file to config.yaml and customize as needed

llm:
  # For Docker Compose: use service name (e.g., http://ollama:11434)
  # For local development: use localhost (e.g., http://localhost:11434)
  endpoint: "http://localhost:11434"
  model: "llama3.1:8b"
  timeout: 120

vault:
  path: "/path/to/obsidian/vault"
  recipes_dir: "personal/recipes"

log_level: "INFO"

# Alternative: You can also set LLM_BASE_URL environment variable
# Example: LLM_BASE_URL=http://ollama:11434
# This will override the llm.endpoint value above

